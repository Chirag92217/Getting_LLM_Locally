Setting Up LLaVA Locally (13B)

LLaVA is an open-source multimodal LLM that can process both images and text. It combines a vision encoder (e.g., CLIP) with a LLaMA-based language model for visual question answering and more.

Hardware Requirements:

1.GPU: NVIDIA RTX 3090, 4090, or A100 recommended
(For LLaVA 13B, minimum: 24GB VRAM; recommended: 48GB VRAM or model sharding across multiple GPUs)

2.RAM: At least 32GB system memory

3.Disk: ~50GB free storage (for model weights, dependencies, and image processing)
